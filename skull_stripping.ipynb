{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skull_stripping",
      "provenance": [],
      "authorship_tag": "ABX9TyOdh6X0fBcqMUIk2ylZOuwS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIIljEKfPipS"
      },
      "source": [
        "!pip install --upgrade nibabel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWjslM5C7fsb"
      },
      "source": [
        "Get data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJkyvhVv7dXu"
      },
      "source": [
        "!wget \"https://putpoznanpl-my.sharepoint.com/:u:/g/personal/dominik_pieczynski_put_poznan_pl/EWIZ_xm8wXpMjQDgF2VQ1csB4QuHPKoj5vDpj6CQi9p-AA?e=yQr6fn&download=1\" -O public.zip\n",
        "!unzip -q public.zip\n",
        "!rm public.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRRjwPzY7jdk"
      },
      "source": [
        "Create valid dirs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8u0p9IG7i0r"
      },
      "source": [
        "!mkdir /content/FirstDataset/valid\n",
        "!mkdir /content/SecondDataset/valid"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dOQI1hd7pwy"
      },
      "source": [
        "Checking the number of scans in dirs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNW-oJqM7oVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a59f336-d638-443a-affe-9c9c4e8d699d"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "print(\"FirstDataset\")\n",
        "filepath1 = '/content/FirstDataset/train'\n",
        "filenames1 = os.listdir(filepath1)\n",
        "filenames1 = sorted(filenames1)\n",
        "print('train', len(filenames1))\n",
        "print('test', len(os.listdir('/content/FirstDataset/test')))\n",
        "print(int(len(filenames1)/5)) #114\n",
        "\n",
        "print(\"SecondDataset\")\n",
        "filepath2 = '/content/SecondDataset/train'\n",
        "filenames2 = os.listdir(filepath2)\n",
        "filenames2 = sorted(filenames2)\n",
        "print('train', len(filenames2))\n",
        "print('test', len(os.listdir('/content/SecondDataset/test')))\n",
        "print(int(len(filenames2)/5)) # 20"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FirstDataset\n",
            "train 574\n",
            "test 72\n",
            "114\n",
            "SecondDataset\n",
            "train 100\n",
            "test 25\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDDLBtQe73iy"
      },
      "source": [
        "Transfer of some data from train dir to valid dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbG-rMgq726q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048f1325-fe0c-497c-d223-9ba58adb2894"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# FirstDataset\n",
        "filepath1 = '/content/FirstDataset/train'\n",
        "filenames1 = os.listdir(filepath1)\n",
        "filenames1 = sorted(filenames1)\n",
        "# print(len(filenames1))\n",
        "valid_filenames1 = filenames1[:int(len(filenames1)/5)]\n",
        "# print(valid_filenames1)\n",
        "for file_name in valid_filenames1:\n",
        "  shutil.move(f\"/content/FirstDataset/train/{file_name}\", f\"/content/FirstDataset/valid/{file_name}\")\n",
        "print(\"train len:\", len(os.listdir('/content/FirstDataset/train')))\n",
        "print(\"valid len:\", len(os.listdir('/content/FirstDataset/valid')))\n",
        "print('test len', len(os.listdir('/content/FirstDataset/test')))\n",
        "print('finish 1')\n",
        "\n",
        "# SecondDataset\n",
        "filepath2 = '/content/SecondDataset/train'\n",
        "filenames2 = os.listdir(filepath2)\n",
        "filenames2 = sorted(filenames2)\n",
        "# print(len(filenames2))\n",
        "valid_filenames2 = filenames2[:int(len(filenames2)/5)]\n",
        "# print(valid_filenames2)\n",
        "for file_name in valid_filenames2:\n",
        "  shutil.move(f\"/content/SecondDataset/train/{file_name}\", f\"/content/SecondDataset/valid/{file_name}\")\n",
        "print(\"train len:\", len(os.listdir('/content/SecondDataset/train')))\n",
        "print(\"valid len:\", len(os.listdir('/content/SecondDataset/valid')))\n",
        "print('test len', len(os.listdir('/content/SecondDataset/test')))\n",
        "print('finish 2')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train len: 460\n",
            "valid len: 114\n",
            "test len 72\n",
            "finish 1\n",
            "train len: 80\n",
            "valid len: 20\n",
            "test len 25\n",
            "finish 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDPrcVQzy34B"
      },
      "source": [
        "###Eksport to png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2Qg0vUbzbja"
      },
      "source": [
        "Template methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grIXGjAEza4Q"
      },
      "source": [
        "import numpy as np\n",
        "import nibabel as nib\n",
        "\n",
        "from typing import Tuple, List\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def load_raw_volume(path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
        "  data: nib.Nifti1Image = nib.load(str(path))\n",
        "  data = nib.as_closest_canonical(data)\n",
        "  raw_data = data.get_fdata(caching='unchanged', dtype=np.float32)\n",
        "  return raw_data, data.affine\n",
        "\n",
        "\n",
        "def load_labels_volume(path: Path) -> np.ndarray:\n",
        "  return load_raw_volume(path)[0].astype(np.uint8)\n",
        "\n",
        "\n",
        "def save_labels(data: np.ndarray, affine: np.ndarray, path: Path):\n",
        "  nib.save(nib.Nifti1Image(data, affine), str(path))\n",
        "\n",
        "\n",
        "def show_slices(slices: List[np.ndarray]):\n",
        "   fig, axes = plt.subplots(1, len(slices))\n",
        "   for i, data_slice in enumerate(slices):\n",
        "       axes[i].imshow(data_slice.T, cmap=\"gray\", origin=\"lower\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNh14hLCy6iW"
      },
      "source": [
        "Create dirs for images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPEbLLXBy3qQ"
      },
      "source": [
        "!rm -rf FirstDataset_images\n",
        "\n",
        "\n",
        "!mkdir /content/FirstDataset_images\n",
        "\n",
        "!mkdir /content/FirstDataset_images/train\n",
        "!mkdir /content/FirstDataset_images/train/images\n",
        "!mkdir /content/FirstDataset_images/train/masks\n",
        "\n",
        "!mkdir /content/FirstDataset_images/valid\n",
        "!mkdir /content/FirstDataset_images/valid/images\n",
        "!mkdir /content/FirstDataset_images/valid/masks"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6AGYKrR4c6i"
      },
      "source": [
        "Export to png"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKfmAV5P0Cdh"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "\n",
        "first_dataset_path = Path('/content/FirstDataset/train')\n",
        "first_image_dataset_path = Path('/content/FirstDataset_images/train/images')\n",
        "first_mask_dataset_path = Path('/content/FirstDataset_images/train/masks')\n",
        "\n",
        "\n",
        "for scan_path in first_dataset_path.iterdir():\n",
        "  print(scan_path)\n",
        "  raw_volume, affine = load_raw_volume(scan_path)\n",
        "  mask_volume = load_labels_volume(scan_path)\n",
        "  if scan_path.name.endswith('mask.nii.gz'):\n",
        "    for idx in range(mask_volume.shape[0]):\n",
        "      path = f'/content/FirstDataset_images/train/masks/{scan_path.name}_{idx}_.png'\n",
        "      plt.imsave(path, mask_volume[idx])\n",
        "  else:\n",
        "    for idx in range(raw_volume.shape[0]):\n",
        "      path = f'/content/FirstDataset_images/train/images/{scan_path.name}_{idx}_.png'\n",
        "      plt.imsave(path, raw_volume[idx])\n",
        "\n",
        "\n",
        "print(\"TRAIN DATASET FINISHED\")\n",
        "\n",
        "first_dataset_path = Path('/content/FirstDataset/valid')\n",
        "first_image_dataset_path_valid = Path('/content/FirstDataset_images/valid/images')\n",
        "first_mask_dataset_path_valid = Path('/content/FirstDataset_images/valid/masks')\n",
        "\n",
        "for scan_path in first_dataset_path.iterdir():\n",
        "  print(scan_path)\n",
        "  raw_volume, affine = load_raw_volume(scan_path)\n",
        "  mask_volume = load_labels_volume(scan_path)\n",
        "  if scan_path.name.endswith('mask.nii.gz'):\n",
        "    for idx in range(mask_volume.shape[0]):\n",
        "      path = f'/content/FirstDataset_images/valid/masks/{scan_path.name}_{idx}_.png'\n",
        "      plt.imsave(path, mask_volume[idx])\n",
        "  else:\n",
        "    for idx in range(raw_volume.shape[0]):\n",
        "      path = f'/content/FirstDataset_images/valid/images/{scan_path.name}_{idx}_.png'\n",
        "      plt.imsave(path, raw_volume[idx])\n",
        "\n",
        "\n",
        "print(\"VALID DATASET FINISHED\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}